### api_server and background image have to be rebuilt locally, please run:
# docker compose build api_server background
# docker compose up -d
IMAGE_TAG=latest

### Edit and enable this section if you use a local Unstructured provider
### An empty string will point to the cloud endpoint of the Unstructured service
#UNSTRUCTURED_API_URL="http://localhost:8000/general/v0/general"
UNSTRUCTURED_API_URL="http://host.docker.internal:8000"
UNSTRUCTURED_STRATEGY="hi_res"
#UNSTRUCTURED_HI_RES_MODEL_NAME="chipper"
UNSTRUCTURED_LANGUAGES="en,it"
UNSTRUCTURED_COORDINATES="true"
UNSTRUCTURED_INCLUDE_PAGE_BREAKS="false"
UNSTRUCTURED_UNIQUE_ELEMENT_IDS="true"
UNSTRUCTURED_MULTIPAGE_SECTIONS="true"
UNSTRUCTURED_COMBINE_UNDER_N_CHARS=""
UNSTRUCTURED_MAX_CHARACTERS="500"
UNSTRUCTURED_NEW_AFTER_N_CHARS=""
UNSTRUCTURED_OVERLAP="0"
UNSTRUCTURED_OVERLAP_ALL="false"
UNSTRUCTURED_PDF_INFER_TABLE_STRUCTURE="true"
UNSTRUCTURED_SKIP_INFER_TABLE_TYPES=""
#UNSTRUCTURED_INCLUDE_SLIDE_NOTES="true"
UNSTRUCTURED_EXTRACT_IMAGE_BLOCK_TYPES=""

### Edit and enable this section if you use an external llama.cpp inference provider
LITELLM_MODEL_ALIAS={"openai/phi-4":"openai//home/carlo/Documenti/AI/models/phi-4-Q6_K.gguf"}

### Enable this section if your HW is old and slow
QA_TIMEOUT=1200
TF_DR_TIMEOUT_LONG=1200
TF_DR_TIMEOUT_SHORT=600
AGENT_TIMEOUT_LLM_GENERAL_GENERATION=1200
AGENT_TIMEOUT_CONNECT_LLM_GENERAL_GENERATION=1200
AGENT_TIMEOUT_LLM_INITIAL_ANSWER_GENERATION=1200
AGENT_TIMEOUT_CONNECT_LLM_INITIAL_ANSWER_GENERATION=1200
AGENT_TIMEOUT_LLM_REFINED_ANSWER_GENERATION=1200
AGENT_TIMEOUT_CONNECT_LLM_REFINED_ANSWER_GENERATION=1200
