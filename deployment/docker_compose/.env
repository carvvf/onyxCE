### api_server and background image have to be rebuilt locally, please run:
# docker compose build api_server # do NOT build api_server and background at the same time!
# docker compose build background
# docker compose up -d

### This .env file is specific to docker-compose.yml

IMAGE_TAG=latest

### Edit and enable this section if you use a local Unstructured provider
### An empty string will point to the cloud endpoint of the Unstructured service
#UNSTRUCTURED_API_URL="http://localhost:8000/general/v0/general"
UNSTRUCTURED_API_URL="http://host.docker.internal:8000"
UNSTRUCTURED_STRATEGY="hi_res"
#UNSTRUCTURED_HI_RES_MODEL_NAME="chipper"
UNSTRUCTURED_LANGUAGES="en,it"
UNSTRUCTURED_COORDINATES="true"
UNSTRUCTURED_INCLUDE_PAGE_BREAKS="false"
UNSTRUCTURED_UNIQUE_ELEMENT_IDS="true"
UNSTRUCTURED_MULTIPAGE_SECTIONS="true"
UNSTRUCTURED_COMBINE_UNDER_N_CHARS=""
UNSTRUCTURED_MAX_CHARACTERS="500"
UNSTRUCTURED_NEW_AFTER_N_CHARS=""
UNSTRUCTURED_OVERLAP="0"
UNSTRUCTURED_OVERLAP_ALL="false"
UNSTRUCTURED_PDF_INFER_TABLE_STRUCTURE="true"
UNSTRUCTURED_SKIP_INFER_TABLE_TYPES=""
#UNSTRUCTURED_INCLUDE_SLIDE_NOTES="true"
UNSTRUCTURED_EXTRACT_IMAGE_BLOCK_TYPES=""

### Edit and enable this section if you use an external llama.cpp inference provider
#LITELLM_MODEL_ALIAS={"openai/phi-4":"openai//home/carlo/Documenti/AI/models/phi-4-Q6_K.gguf"}
LITELLM_MODEL_ALIAS={"openai/phi-4":"openai//home/carlo/Documenti/AI/models/llm/phi-4-Q8_0.gguf"}

### Edit this section according to your HW capability (timeout in seconds)
# Increase chat answer timeout if your LLM is slow
CUSTOM_TOUT_LONG=2400
CUSTOM_TOUT_SHORT=1200
# Increase request timeout for Unstructured hi_res processing
REQUEST_TIMEOUT_SECONDS=1200
NGINX_TIMEOUT=2400
